{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from keras.layers import Dense, Input, Dropout, BatchNormalization, Activation\n",
    "from keras.models import Model, Sequential\n",
    "from keras import regularizers\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import StandardScaler, LabelBinarizer\n",
    "import collections\n",
    "from tsfresh import extract_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfTrainData = pd.read_csv(\"training_set.csv\")\n",
    "dfMetaData = pd.read_csv(\"training_set_metadata.csv\")\n",
    "dfPoly = pd.read_csv(\"polynomial_coefficients_train.csv\")\n",
    "dfBaseline = dfMetaData.drop([\"distmod\",\"object_id\"],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Feature Extraction: 100%|██████████| 10/10 [00:37<00:00,  3.04s/it]\n"
     ]
    }
   ],
   "source": [
    "def feature_transform(dfData,dfMetaData,dfPoly):\n",
    "    dfFeatures = pd.DataFrame()\n",
    "\n",
    "    #calculate summary statistics for flux\n",
    "    grouped_data = dfData.groupby(by = 'object_id')\n",
    "    dfFeatures[\"mean_flux\"] = grouped_data['flux'].mean()\n",
    "    dfFeatures[\"observation_count\"] = grouped_data['flux'].count()\n",
    "    dfFeatures[\"median_flux\"] = grouped_data['flux'].median()\n",
    "    dfFeatures[\"max_flux\"] = grouped_data['flux'].max()\n",
    "    dfFeatures[\"min_flux\"] = grouped_data['flux'].min()\n",
    "    dfFeatures[\"std_flux\"] = grouped_data['flux'].std()\n",
    "\n",
    "    #differentiate between cyclic and one-time events, feature given by kaggle discussion linked below\n",
    "    #https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69696#410538\n",
    "    grouped_data_detected = dfData.loc[dfData[\"detected\"] ==1].groupby(by = 'object_id')\n",
    "    dfFeatures[\"detected_timediff\"] = grouped_data_detected['mjd'].max() - grouped_data_detected['mjd'].min()\n",
    "\n",
    "    #calculate time series features from tsfresh library \n",
    "    tsfresh_features =  {\n",
    "        'fft_coefficient': [{'coeff': 0, 'attr':'abs'}],\n",
    "        'kurtosis' : None, \n",
    "        'skewness' : None,\n",
    "        'c3' : [{'lag' :10}],\n",
    "    }                \n",
    "    features_temp = extract_features(dfData, column_id='object_id', column_sort='mjd', column_kind='passband', column_value='flux', default_fc_parameters=tsfresh_features)\n",
    "    dfFeatures = dfFeatures.merge(right = features_temp, left_index = True, right_index = True)\n",
    "\n",
    "    dfFeatures = dfFeatures.merge(right = dfMetaData, left_index = True, right_on = 'object_id')\n",
    "    dfFeatures.fillna(0,inplace = True)\n",
    "    \n",
    "    return dfFeatures\n",
    "\n",
    "dfFeatures = feature_transform(dfTrainData,dfMetaData,dfPoly)\n",
    "dfMerged = dfFeatures.merge(dfPoly,left_on = 'object_id',right_on = \"object_id\")\n",
    "dfMerged = dfMerged.drop([\"distmod\",\"object_id\"],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda/lib/python3.6/site-packages/sklearn/preprocessing/data.py:625: DataConversionWarning: Data with input dtype int64, float64, object were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "/anaconda/lib/python3.6/site-packages/sklearn/base.py:462: DataConversionWarning: Data with input dtype int64, float64, object were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n",
      "/anaconda/lib/python3.6/site-packages/ipykernel_launcher.py:12: DataConversionWarning: Data with input dtype int64, float64, object were all converted to float64 by StandardScaler.\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#preprocessing \n",
    "dfTrain, dfTest = train_test_split(dfMerged)\n",
    "\n",
    "y_train = dfTrain[\"target\"]\n",
    "X_train = dfTrain.drop([\"target\"],axis = 1)\n",
    "\n",
    "y_test = dfTest[\"target\"]\n",
    "X_test = dfTest.drop([\"target\"],axis = 1)\n",
    "\n",
    "normalizer = StandardScaler()\n",
    "X_train = normalizer.fit_transform(X_train)\n",
    "X_test = normalizer.transform(X_test)\n",
    "\n",
    "encoder = LabelBinarizer()\n",
    "y_categorical = encoder.fit_transform(y_train)\n",
    "y_categorical_test = encoder.transform(y_test)\n",
    "\n",
    "labels, counts = np.unique(y_train,return_counts = True)\n",
    "wtable = counts / y_train.shape[0]\n",
    "\n",
    "#multi weighted logloss used for competition, function taken from kaggle discussion below\n",
    "#https://www.kaggle.com/c/PLAsTiCC-2018/discussion/69795\n",
    "def multi_weighted_logloss(y_true,y_pred):  \n",
    "    yc=tf.clip_by_value(y_pred,1e-15,1-1e-15)\n",
    "    loss=-(tf.reduce_mean(tf.reduce_mean(y_true*tf.log(yc),axis=0)/wtable))\n",
    "    return loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 5886 samples, validate on 1962 samples\n",
      "Epoch 1/100\n",
      "5886/5886 [==============================] - 5s 861us/step - loss: 2.6103 - acc: 0.1882 - val_loss: 1.8547 - val_acc: 0.3609\n",
      "Epoch 2/100\n",
      "5886/5886 [==============================] - 1s 97us/step - loss: 1.8443 - acc: 0.3070 - val_loss: 1.5519 - val_acc: 0.4185\n",
      "Epoch 3/100\n",
      "5886/5886 [==============================] - 1s 99us/step - loss: 1.6163 - acc: 0.3617 - val_loss: 1.4107 - val_acc: 0.4439\n",
      "Epoch 4/100\n",
      "5886/5886 [==============================] - 1s 103us/step - loss: 1.4644 - acc: 0.4069 - val_loss: 1.3244 - val_acc: 0.4796\n",
      "Epoch 5/100\n",
      "5886/5886 [==============================] - 1s 115us/step - loss: 1.3907 - acc: 0.4346 - val_loss: 1.2784 - val_acc: 0.4709\n",
      "Epoch 6/100\n",
      "5886/5886 [==============================] - 1s 145us/step - loss: 1.2855 - acc: 0.4484 - val_loss: 1.2092 - val_acc: 0.5056\n",
      "Epoch 7/100\n",
      "5886/5886 [==============================] - 1s 110us/step - loss: 1.2520 - acc: 0.4618 - val_loss: 1.1965 - val_acc: 0.5046\n",
      "Epoch 8/100\n",
      "5886/5886 [==============================] - 1s 150us/step - loss: 1.2146 - acc: 0.4818 - val_loss: 1.1679 - val_acc: 0.5301\n",
      "Epoch 9/100\n",
      "5886/5886 [==============================] - 1s 140us/step - loss: 1.1715 - acc: 0.4896 - val_loss: 1.1445 - val_acc: 0.5194\n",
      "Epoch 10/100\n",
      "5886/5886 [==============================] - 1s 136us/step - loss: 1.1568 - acc: 0.4942 - val_loss: 1.1033 - val_acc: 0.5459\n",
      "Epoch 11/100\n",
      "5886/5886 [==============================] - 1s 126us/step - loss: 1.1211 - acc: 0.5102 - val_loss: 1.1236 - val_acc: 0.5510\n",
      "Epoch 12/100\n",
      "5886/5886 [==============================] - 1s 133us/step - loss: 1.1052 - acc: 0.5073 - val_loss: 1.1215 - val_acc: 0.5571\n",
      "Epoch 13/100\n",
      "5886/5886 [==============================] - 1s 122us/step - loss: 1.0721 - acc: 0.5192 - val_loss: 1.1095 - val_acc: 0.5398\n",
      "Epoch 14/100\n",
      "5886/5886 [==============================] - 1s 120us/step - loss: 1.0689 - acc: 0.5267 - val_loss: 1.0555 - val_acc: 0.5576\n",
      "Epoch 15/100\n",
      "5886/5886 [==============================] - 1s 106us/step - loss: 1.0311 - acc: 0.5285 - val_loss: 1.0550 - val_acc: 0.5556\n",
      "Epoch 16/100\n",
      "5886/5886 [==============================] - 1s 108us/step - loss: 1.0258 - acc: 0.5343 - val_loss: 1.0357 - val_acc: 0.5520\n",
      "Epoch 17/100\n",
      "5886/5886 [==============================] - 1s 174us/step - loss: 1.0146 - acc: 0.5311 - val_loss: 1.0859 - val_acc: 0.5815\n",
      "Epoch 18/100\n",
      "5886/5886 [==============================] - 1s 133us/step - loss: 0.9984 - acc: 0.5406 - val_loss: 1.0907 - val_acc: 0.5668\n",
      "Epoch 19/100\n",
      "5886/5886 [==============================] - 1s 117us/step - loss: 0.9997 - acc: 0.5440 - val_loss: 1.0109 - val_acc: 0.5770\n",
      "Epoch 20/100\n",
      "5886/5886 [==============================] - 1s 103us/step - loss: 0.9842 - acc: 0.5511 - val_loss: 0.9805 - val_acc: 0.5642\n",
      "Epoch 21/100\n",
      "5886/5886 [==============================] - 1s 103us/step - loss: 0.9532 - acc: 0.5511 - val_loss: 1.0072 - val_acc: 0.5805\n",
      "Epoch 22/100\n",
      "5886/5886 [==============================] - 1s 113us/step - loss: 0.9569 - acc: 0.5527 - val_loss: 0.9991 - val_acc: 0.5775\n",
      "Epoch 23/100\n",
      "5886/5886 [==============================] - 1s 126us/step - loss: 0.9523 - acc: 0.5642 - val_loss: 0.9907 - val_acc: 0.5846\n",
      "Epoch 24/100\n",
      "5886/5886 [==============================] - 1s 201us/step - loss: 0.9420 - acc: 0.5554 - val_loss: 0.9718 - val_acc: 0.5805\n",
      "Epoch 25/100\n",
      "5886/5886 [==============================] - 1s 128us/step - loss: 0.9097 - acc: 0.5742 - val_loss: 0.9971 - val_acc: 0.6081\n",
      "Epoch 26/100\n",
      "5886/5886 [==============================] - 1s 114us/step - loss: 0.9173 - acc: 0.5688 - val_loss: 0.9716 - val_acc: 0.5984\n",
      "Epoch 27/100\n",
      "5886/5886 [==============================] - 1s 142us/step - loss: 0.9271 - acc: 0.5766 - val_loss: 1.0200 - val_acc: 0.5861\n",
      "Epoch 28/100\n",
      "5886/5886 [==============================] - 1s 127us/step - loss: 0.9194 - acc: 0.5657 - val_loss: 0.9925 - val_acc: 0.6035\n",
      "Epoch 29/100\n",
      "5886/5886 [==============================] - 1s 135us/step - loss: 0.9278 - acc: 0.5613 - val_loss: 0.9721 - val_acc: 0.5938\n",
      "Epoch 30/100\n",
      "5886/5886 [==============================] - 1s 113us/step - loss: 0.9084 - acc: 0.5731 - val_loss: 0.9853 - val_acc: 0.6065\n",
      "Epoch 31/100\n",
      "5886/5886 [==============================] - 1s 104us/step - loss: 0.9092 - acc: 0.5758 - val_loss: 0.9911 - val_acc: 0.6106\n",
      "Epoch 32/100\n",
      "5886/5886 [==============================] - 1s 101us/step - loss: 0.8965 - acc: 0.5751 - val_loss: 0.9398 - val_acc: 0.5917\n",
      "Epoch 33/100\n",
      "5886/5886 [==============================] - 1s 101us/step - loss: 0.9032 - acc: 0.5792 - val_loss: 0.9158 - val_acc: 0.6019\n",
      "Epoch 34/100\n",
      "5886/5886 [==============================] - 1s 100us/step - loss: 0.8792 - acc: 0.5780 - val_loss: 0.9315 - val_acc: 0.6055\n",
      "Epoch 35/100\n",
      "5886/5886 [==============================] - 1s 101us/step - loss: 0.8738 - acc: 0.5802 - val_loss: 0.8995 - val_acc: 0.6121\n",
      "Epoch 36/100\n",
      "5886/5886 [==============================] - 1s 100us/step - loss: 0.8699 - acc: 0.5800 - val_loss: 0.9185 - val_acc: 0.5963\n",
      "Epoch 37/100\n",
      "5886/5886 [==============================] - 1s 102us/step - loss: 0.8652 - acc: 0.5839 - val_loss: 0.9849 - val_acc: 0.5897\n",
      "Epoch 38/100\n",
      "5886/5886 [==============================] - 1s 102us/step - loss: 0.8483 - acc: 0.5890 - val_loss: 0.9297 - val_acc: 0.6086\n",
      "Epoch 39/100\n",
      "5886/5886 [==============================] - 1s 110us/step - loss: 0.8713 - acc: 0.5855 - val_loss: 0.9665 - val_acc: 0.6147\n",
      "Epoch 40/100\n",
      "5886/5886 [==============================] - 1s 112us/step - loss: 0.8399 - acc: 0.5967 - val_loss: 0.9243 - val_acc: 0.6035\n",
      "Epoch 41/100\n",
      "5886/5886 [==============================] - 1s 101us/step - loss: 0.8448 - acc: 0.5844 - val_loss: 0.9899 - val_acc: 0.6019\n",
      "Epoch 42/100\n",
      "5886/5886 [==============================] - 1s 101us/step - loss: 0.8468 - acc: 0.5858 - val_loss: 0.9515 - val_acc: 0.6081\n",
      "Epoch 43/100\n",
      "5886/5886 [==============================] - 1s 100us/step - loss: 0.8384 - acc: 0.5899 - val_loss: 0.9519 - val_acc: 0.6116\n",
      "Epoch 44/100\n",
      "5886/5886 [==============================] - 1s 101us/step - loss: 0.8424 - acc: 0.5866 - val_loss: 0.9880 - val_acc: 0.6152\n",
      "Epoch 45/100\n",
      "5886/5886 [==============================] - 1s 99us/step - loss: 0.8151 - acc: 0.5917 - val_loss: 0.9577 - val_acc: 0.6101\n",
      "Epoch 46/100\n",
      "5886/5886 [==============================] - 1s 103us/step - loss: 0.8260 - acc: 0.5897 - val_loss: 0.9629 - val_acc: 0.6218\n",
      "Epoch 47/100\n",
      "5886/5886 [==============================] - 1s 110us/step - loss: 0.8339 - acc: 0.6019 - val_loss: 0.9341 - val_acc: 0.5989\n",
      "Epoch 48/100\n",
      "5886/5886 [==============================] - 1s 103us/step - loss: 0.8166 - acc: 0.5912 - val_loss: 0.9707 - val_acc: 0.6162\n",
      "Epoch 49/100\n",
      "5886/5886 [==============================] - 1s 103us/step - loss: 0.8574 - acc: 0.5895 - val_loss: 0.9826 - val_acc: 0.6055\n",
      "Epoch 50/100\n",
      "5886/5886 [==============================] - 1s 103us/step - loss: 0.8274 - acc: 0.5885 - val_loss: 1.0019 - val_acc: 0.5948\n",
      "Epoch 51/100\n",
      "5886/5886 [==============================] - 1s 99us/step - loss: 0.8179 - acc: 0.5938 - val_loss: 0.9185 - val_acc: 0.5989\n",
      "Epoch 52/100\n",
      "5886/5886 [==============================] - 1s 101us/step - loss: 0.8256 - acc: 0.5878 - val_loss: 0.9208 - val_acc: 0.6177\n",
      "Epoch 53/100\n",
      "5886/5886 [==============================] - 1s 101us/step - loss: 0.8235 - acc: 0.5943 - val_loss: 0.9890 - val_acc: 0.6050\n",
      "Epoch 54/100\n",
      "5886/5886 [==============================] - 1s 100us/step - loss: 0.7923 - acc: 0.6050 - val_loss: 1.0550 - val_acc: 0.6040\n",
      "Epoch 55/100\n",
      "5886/5886 [==============================] - 1s 97us/step - loss: 0.7957 - acc: 0.6028 - val_loss: 0.9728 - val_acc: 0.6157\n",
      "Epoch 56/100\n",
      "5886/5886 [==============================] - 1s 103us/step - loss: 0.8028 - acc: 0.6021 - val_loss: 0.8866 - val_acc: 0.6096\n",
      "Epoch 57/100\n",
      "5886/5886 [==============================] - 1s 99us/step - loss: 0.7929 - acc: 0.6018 - val_loss: 0.8824 - val_acc: 0.6213\n",
      "Epoch 58/100\n",
      "5886/5886 [==============================] - 1s 99us/step - loss: 0.7991 - acc: 0.5946 - val_loss: 0.9042 - val_acc: 0.6172\n",
      "Epoch 59/100\n",
      "5886/5886 [==============================] - 1s 99us/step - loss: 0.7984 - acc: 0.5999 - val_loss: 0.8586 - val_acc: 0.6116\n",
      "Epoch 60/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5886/5886 [==============================] - 1s 118us/step - loss: 0.8140 - acc: 0.6072 - val_loss: 0.8479 - val_acc: 0.6188\n",
      "Epoch 61/100\n",
      "5886/5886 [==============================] - 1s 150us/step - loss: 0.8006 - acc: 0.6030 - val_loss: 0.8395 - val_acc: 0.6274\n",
      "Epoch 62/100\n",
      "5886/5886 [==============================] - 1s 137us/step - loss: 0.7813 - acc: 0.6116 - val_loss: 0.8266 - val_acc: 0.6147\n",
      "Epoch 63/100\n",
      "5886/5886 [==============================] - 1s 133us/step - loss: 0.7768 - acc: 0.6014 - val_loss: 0.9105 - val_acc: 0.6142\n",
      "Epoch 64/100\n",
      "5886/5886 [==============================] - 1s 143us/step - loss: 0.7785 - acc: 0.6206 - val_loss: 0.9185 - val_acc: 0.6228\n",
      "Epoch 65/100\n",
      "5886/5886 [==============================] - 1s 131us/step - loss: 0.7935 - acc: 0.5980 - val_loss: 0.8570 - val_acc: 0.6239\n",
      "Epoch 66/100\n",
      "5886/5886 [==============================] - 1s 122us/step - loss: 0.7650 - acc: 0.6104 - val_loss: 0.8740 - val_acc: 0.6086\n",
      "Epoch 67/100\n",
      "5886/5886 [==============================] - 1s 110us/step - loss: 0.7933 - acc: 0.6030 - val_loss: 0.8697 - val_acc: 0.6065\n",
      "Epoch 68/100\n",
      "5886/5886 [==============================] - 1s 100us/step - loss: 0.7958 - acc: 0.6140 - val_loss: 0.8723 - val_acc: 0.6269\n",
      "Epoch 69/100\n",
      "5886/5886 [==============================] - 1s 100us/step - loss: 0.7639 - acc: 0.6145 - val_loss: 0.8867 - val_acc: 0.6233\n",
      "Epoch 70/100\n",
      "5886/5886 [==============================] - 1s 99us/step - loss: 0.7751 - acc: 0.6047 - val_loss: 0.8876 - val_acc: 0.6233\n",
      "Epoch 71/100\n",
      "5886/5886 [==============================] - 1s 98us/step - loss: 0.7588 - acc: 0.6082 - val_loss: 0.8280 - val_acc: 0.6310\n",
      "Epoch 72/100\n",
      "5886/5886 [==============================] - 1s 102us/step - loss: 0.7595 - acc: 0.6131 - val_loss: 0.9157 - val_acc: 0.5984\n",
      "Epoch 73/100\n",
      "5886/5886 [==============================] - 1s 98us/step - loss: 0.7481 - acc: 0.6150 - val_loss: 0.8823 - val_acc: 0.6101\n",
      "Epoch 74/100\n",
      "5886/5886 [==============================] - 1s 100us/step - loss: 0.7614 - acc: 0.6081 - val_loss: 0.8815 - val_acc: 0.6096\n",
      "Epoch 75/100\n",
      "5886/5886 [==============================] - 1s 101us/step - loss: 0.7605 - acc: 0.6215 - val_loss: 0.8747 - val_acc: 0.6045\n",
      "Epoch 76/100\n",
      "5886/5886 [==============================] - 1s 108us/step - loss: 0.7602 - acc: 0.6181 - val_loss: 0.8538 - val_acc: 0.6014\n",
      "Epoch 77/100\n",
      "5886/5886 [==============================] - 1s 100us/step - loss: 0.7453 - acc: 0.6186 - val_loss: 0.8617 - val_acc: 0.6335\n",
      "Epoch 78/100\n",
      "5886/5886 [==============================] - 1s 100us/step - loss: 0.7686 - acc: 0.6157 - val_loss: 0.8638 - val_acc: 0.6167\n",
      "Epoch 79/100\n",
      "5886/5886 [==============================] - 1s 100us/step - loss: 0.7477 - acc: 0.6109 - val_loss: 0.8956 - val_acc: 0.6249\n",
      "Epoch 80/100\n",
      "5886/5886 [==============================] - 1s 102us/step - loss: 0.7470 - acc: 0.6335 - val_loss: 0.8799 - val_acc: 0.6126\n",
      "Epoch 81/100\n",
      "5886/5886 [==============================] - 1s 101us/step - loss: 0.7515 - acc: 0.6064 - val_loss: 0.8937 - val_acc: 0.6356\n",
      "Epoch 82/100\n",
      "5886/5886 [==============================] - 1s 100us/step - loss: 0.7384 - acc: 0.6239 - val_loss: 0.9326 - val_acc: 0.6035\n",
      "Epoch 83/100\n",
      "5886/5886 [==============================] - 1s 101us/step - loss: 0.7413 - acc: 0.6262 - val_loss: 0.9045 - val_acc: 0.6014\n",
      "Epoch 84/100\n",
      "5886/5886 [==============================] - 1s 99us/step - loss: 0.7557 - acc: 0.6082 - val_loss: 0.8273 - val_acc: 0.6223\n",
      "Epoch 85/100\n",
      "5886/5886 [==============================] - 1s 101us/step - loss: 0.7352 - acc: 0.6237 - val_loss: 0.8793 - val_acc: 0.6177\n",
      "Epoch 86/100\n",
      "5886/5886 [==============================] - 1s 99us/step - loss: 0.7341 - acc: 0.6306 - val_loss: 0.8723 - val_acc: 0.6295\n",
      "Epoch 87/100\n",
      "5886/5886 [==============================] - 1s 102us/step - loss: 0.7408 - acc: 0.6157 - val_loss: 0.9258 - val_acc: 0.6203\n",
      "Epoch 88/100\n",
      "5886/5886 [==============================] - 1s 106us/step - loss: 0.7304 - acc: 0.6188 - val_loss: 0.9086 - val_acc: 0.6167\n",
      "Epoch 89/100\n",
      "5886/5886 [==============================] - 1s 102us/step - loss: 0.7274 - acc: 0.6291 - val_loss: 0.9124 - val_acc: 0.5989\n",
      "Epoch 90/100\n",
      "5886/5886 [==============================] - 1s 102us/step - loss: 0.7294 - acc: 0.6291 - val_loss: 0.8832 - val_acc: 0.6157\n",
      "Epoch 91/100\n",
      "5886/5886 [==============================] - 1s 101us/step - loss: 0.7310 - acc: 0.6242 - val_loss: 0.9204 - val_acc: 0.6244\n",
      "Epoch 92/100\n",
      "5886/5886 [==============================] - 1s 102us/step - loss: 0.7285 - acc: 0.6174 - val_loss: 0.8551 - val_acc: 0.6239\n",
      "Epoch 93/100\n",
      "5886/5886 [==============================] - 1s 100us/step - loss: 0.7117 - acc: 0.6320 - val_loss: 0.9425 - val_acc: 0.6223\n",
      "Epoch 94/100\n",
      "5886/5886 [==============================] - 1s 172us/step - loss: 0.7263 - acc: 0.6225 - val_loss: 0.9140 - val_acc: 0.6147\n",
      "Epoch 95/100\n",
      "5886/5886 [==============================] - 1s 139us/step - loss: 0.6963 - acc: 0.6323 - val_loss: 0.8932 - val_acc: 0.6366\n",
      "Epoch 96/100\n",
      "5886/5886 [==============================] - 1s 141us/step - loss: 0.7120 - acc: 0.6261 - val_loss: 0.8100 - val_acc: 0.6228\n",
      "Epoch 97/100\n",
      "5886/5886 [==============================] - 1s 128us/step - loss: 0.7101 - acc: 0.6306 - val_loss: 0.8936 - val_acc: 0.6086\n",
      "Epoch 98/100\n",
      "5886/5886 [==============================] - 1s 148us/step - loss: 0.7230 - acc: 0.6283 - val_loss: 0.9155 - val_acc: 0.6427\n",
      "Epoch 99/100\n",
      "5886/5886 [==============================] - 1s 130us/step - loss: 0.7034 - acc: 0.6361 - val_loss: 0.9116 - val_acc: 0.6290\n",
      "Epoch 100/100\n",
      "5886/5886 [==============================] - 1s 111us/step - loss: 0.7040 - acc: 0.6313 - val_loss: 0.9126 - val_acc: 0.6259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1c2481b208>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neurons = 64\n",
    "dropout = 0.125\n",
    "epochs = 100\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(neurons,input_dim = X_train[0].shape[0]))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Dense(neurons))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Dense(neurons))\n",
    "model.add(Activation('elu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Dense(neurons))\n",
    "model.add(Activation('relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Dense(14,activation = \"softmax\"))\n",
    "\n",
    "model.compile(loss = multi_weighted_logloss,optimizer = \"adam\",metrics = ['acc'])\n",
    "\n",
    "model.fit(X_train, y_categorical, batch_size = 128,validation_data=(X_test,y_categorical_test), epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
