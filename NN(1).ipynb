{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pickle\n",
    "import gzip\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Dense, Input, concatenate, Dropout, BatchNormalization, LSTM\n",
    "from keras.models import Model, Sequential\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.preprocessing import MinMaxScaler, OneHotEncoder,StandardScaler, Normalizer\n",
    "import tensorflow as tf\n",
    "import collections "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>variable</th>\n",
       "      <th>b2__change_quantiles__f_agg_\"mean\"__isabs_True__qh_1.0__ql_0.2</th>\n",
       "      <th>b3__change_quantiles__f_agg_\"mean\"__isabs_True__qh_1.0__ql_0.2</th>\n",
       "      <th>b2__change_quantiles__f_agg_\"mean\"__isabs_True__qh_1.0__ql_0.0</th>\n",
       "      <th>b2__mean_abs_change</th>\n",
       "      <th>b2__absolute_sum_of_changes</th>\n",
       "      <th>b3__change_quantiles__f_agg_\"var\"__isabs_False__qh_1.0__ql_0.2</th>\n",
       "      <th>b2__change_quantiles__f_agg_\"mean\"__isabs_True__qh_0.8__ql_0.0</th>\n",
       "      <th>b3__mean_abs_change</th>\n",
       "      <th>b3__change_quantiles__f_agg_\"mean\"__isabs_True__qh_1.0__ql_0.0</th>\n",
       "      <th>b3__absolute_sum_of_changes</th>\n",
       "      <th>...</th>\n",
       "      <th>b4__linear_trend__attr_\"intercept\"</th>\n",
       "      <th>b2__cwt_coefficients__widths_(2, 5, 10, 20)__coeff_9__w_10</th>\n",
       "      <th>b4__number_peaks__n_1</th>\n",
       "      <th>b2__autocorrelation__lag_7</th>\n",
       "      <th>b1__cwt_coefficients__widths_(2, 5, 10, 20)__coeff_14__w_2</th>\n",
       "      <th>b6__cwt_coefficients__widths_(2, 5, 10, 20)__coeff_12__w_20</th>\n",
       "      <th>b3__c3__lag_2</th>\n",
       "      <th>b5__symmetry_looking__r_0.25</th>\n",
       "      <th>b5__symmetry_looking__r_0.30000000000000004</th>\n",
       "      <th>b1__symmetry_looking__r_0.30000000000000004</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>615.0</th>\n",
       "      <td>519.504357</td>\n",
       "      <td>423.716195</td>\n",
       "      <td>647.078397</td>\n",
       "      <td>647.078397</td>\n",
       "      <td>36236.390207</td>\n",
       "      <td>273618.311367</td>\n",
       "      <td>447.784514</td>\n",
       "      <td>503.257899</td>\n",
       "      <td>503.257899</td>\n",
       "      <td>28182.442351</td>\n",
       "      <td>...</td>\n",
       "      <td>-309.031313</td>\n",
       "      <td>-845.660896</td>\n",
       "      <td>16.0</td>\n",
       "      <td>-0.051752</td>\n",
       "      <td>-105.344718</td>\n",
       "      <td>-554.608005</td>\n",
       "      <td>1.525137e+07</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>713.0</th>\n",
       "      <td>2.138922</td>\n",
       "      <td>2.120542</td>\n",
       "      <td>2.241918</td>\n",
       "      <td>2.241918</td>\n",
       "      <td>123.305483</td>\n",
       "      <td>8.156167</td>\n",
       "      <td>2.024384</td>\n",
       "      <td>2.051365</td>\n",
       "      <td>2.051365</td>\n",
       "      <td>112.825074</td>\n",
       "      <td>...</td>\n",
       "      <td>8.056757</td>\n",
       "      <td>14.107800</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.677033</td>\n",
       "      <td>3.142531</td>\n",
       "      <td>11.621645</td>\n",
       "      <td>-8.940954e+01</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>730.0</th>\n",
       "      <td>1.517284</td>\n",
       "      <td>2.015711</td>\n",
       "      <td>1.856014</td>\n",
       "      <td>1.856014</td>\n",
       "      <td>92.800712</td>\n",
       "      <td>16.361706</td>\n",
       "      <td>1.412415</td>\n",
       "      <td>1.888433</td>\n",
       "      <td>1.888433</td>\n",
       "      <td>94.421627</td>\n",
       "      <td>...</td>\n",
       "      <td>-2.154664</td>\n",
       "      <td>-1.488664</td>\n",
       "      <td>17.0</td>\n",
       "      <td>-0.043021</td>\n",
       "      <td>0.221550</td>\n",
       "      <td>-3.167655</td>\n",
       "      <td>2.138440e+02</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>745.0</th>\n",
       "      <td>6.382901</td>\n",
       "      <td>12.654187</td>\n",
       "      <td>8.632928</td>\n",
       "      <td>8.632928</td>\n",
       "      <td>466.178130</td>\n",
       "      <td>1877.644380</td>\n",
       "      <td>1.630198</td>\n",
       "      <td>9.276039</td>\n",
       "      <td>9.276039</td>\n",
       "      <td>500.906092</td>\n",
       "      <td>...</td>\n",
       "      <td>17.777045</td>\n",
       "      <td>-22.191762</td>\n",
       "      <td>15.0</td>\n",
       "      <td>-0.012770</td>\n",
       "      <td>-1.816087</td>\n",
       "      <td>50.504436</td>\n",
       "      <td>6.531625e+03</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1124.0</th>\n",
       "      <td>3.390240</td>\n",
       "      <td>6.607588</td>\n",
       "      <td>2.810284</td>\n",
       "      <td>2.810284</td>\n",
       "      <td>157.375895</td>\n",
       "      <td>388.366325</td>\n",
       "      <td>1.517726</td>\n",
       "      <td>4.686253</td>\n",
       "      <td>4.686253</td>\n",
       "      <td>262.430156</td>\n",
       "      <td>...</td>\n",
       "      <td>-5.657373</td>\n",
       "      <td>1.054442</td>\n",
       "      <td>18.0</td>\n",
       "      <td>0.255567</td>\n",
       "      <td>0.507083</td>\n",
       "      <td>-18.002683</td>\n",
       "      <td>1.037487e+04</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 1902 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "variable  b2__change_quantiles__f_agg_\"mean\"__isabs_True__qh_1.0__ql_0.2  \\\n",
       "id                                                                         \n",
       "615.0                                            519.504357                \n",
       "713.0                                              2.138922                \n",
       "730.0                                              1.517284                \n",
       "745.0                                              6.382901                \n",
       "1124.0                                             3.390240                \n",
       "\n",
       "variable  b3__change_quantiles__f_agg_\"mean\"__isabs_True__qh_1.0__ql_0.2  \\\n",
       "id                                                                         \n",
       "615.0                                            423.716195                \n",
       "713.0                                              2.120542                \n",
       "730.0                                              2.015711                \n",
       "745.0                                             12.654187                \n",
       "1124.0                                             6.607588                \n",
       "\n",
       "variable  b2__change_quantiles__f_agg_\"mean\"__isabs_True__qh_1.0__ql_0.0  \\\n",
       "id                                                                         \n",
       "615.0                                            647.078397                \n",
       "713.0                                              2.241918                \n",
       "730.0                                              1.856014                \n",
       "745.0                                              8.632928                \n",
       "1124.0                                             2.810284                \n",
       "\n",
       "variable  b2__mean_abs_change  b2__absolute_sum_of_changes  \\\n",
       "id                                                           \n",
       "615.0              647.078397                 36236.390207   \n",
       "713.0                2.241918                   123.305483   \n",
       "730.0                1.856014                    92.800712   \n",
       "745.0                8.632928                   466.178130   \n",
       "1124.0               2.810284                   157.375895   \n",
       "\n",
       "variable  b3__change_quantiles__f_agg_\"var\"__isabs_False__qh_1.0__ql_0.2  \\\n",
       "id                                                                         \n",
       "615.0                                         273618.311367                \n",
       "713.0                                              8.156167                \n",
       "730.0                                             16.361706                \n",
       "745.0                                           1877.644380                \n",
       "1124.0                                           388.366325                \n",
       "\n",
       "variable  b2__change_quantiles__f_agg_\"mean\"__isabs_True__qh_0.8__ql_0.0  \\\n",
       "id                                                                         \n",
       "615.0                                            447.784514                \n",
       "713.0                                              2.024384                \n",
       "730.0                                              1.412415                \n",
       "745.0                                              1.630198                \n",
       "1124.0                                             1.517726                \n",
       "\n",
       "variable  b3__mean_abs_change  \\\n",
       "id                              \n",
       "615.0              503.257899   \n",
       "713.0                2.051365   \n",
       "730.0                1.888433   \n",
       "745.0                9.276039   \n",
       "1124.0               4.686253   \n",
       "\n",
       "variable  b3__change_quantiles__f_agg_\"mean\"__isabs_True__qh_1.0__ql_0.0  \\\n",
       "id                                                                         \n",
       "615.0                                            503.257899                \n",
       "713.0                                              2.051365                \n",
       "730.0                                              1.888433                \n",
       "745.0                                              9.276039                \n",
       "1124.0                                             4.686253                \n",
       "\n",
       "variable  b3__absolute_sum_of_changes  \\\n",
       "id                                      \n",
       "615.0                    28182.442351   \n",
       "713.0                      112.825074   \n",
       "730.0                       94.421627   \n",
       "745.0                      500.906092   \n",
       "1124.0                     262.430156   \n",
       "\n",
       "variable                     ...                       \\\n",
       "id                           ...                        \n",
       "615.0                        ...                        \n",
       "713.0                        ...                        \n",
       "730.0                        ...                        \n",
       "745.0                        ...                        \n",
       "1124.0                       ...                        \n",
       "\n",
       "variable  b4__linear_trend__attr_\"intercept\"  \\\n",
       "id                                             \n",
       "615.0                            -309.031313   \n",
       "713.0                               8.056757   \n",
       "730.0                              -2.154664   \n",
       "745.0                              17.777045   \n",
       "1124.0                             -5.657373   \n",
       "\n",
       "variable  b2__cwt_coefficients__widths_(2, 5, 10, 20)__coeff_9__w_10  \\\n",
       "id                                                                     \n",
       "615.0                                           -845.660896            \n",
       "713.0                                             14.107800            \n",
       "730.0                                             -1.488664            \n",
       "745.0                                            -22.191762            \n",
       "1124.0                                             1.054442            \n",
       "\n",
       "variable  b4__number_peaks__n_1  b2__autocorrelation__lag_7  \\\n",
       "id                                                            \n",
       "615.0                      16.0                   -0.051752   \n",
       "713.0                      17.0                    0.677033   \n",
       "730.0                      17.0                   -0.043021   \n",
       "745.0                      15.0                   -0.012770   \n",
       "1124.0                     18.0                    0.255567   \n",
       "\n",
       "variable  b1__cwt_coefficients__widths_(2, 5, 10, 20)__coeff_14__w_2  \\\n",
       "id                                                                     \n",
       "615.0                                           -105.344718            \n",
       "713.0                                              3.142531            \n",
       "730.0                                              0.221550            \n",
       "745.0                                             -1.816087            \n",
       "1124.0                                             0.507083            \n",
       "\n",
       "variable  b6__cwt_coefficients__widths_(2, 5, 10, 20)__coeff_12__w_20  \\\n",
       "id                                                                      \n",
       "615.0                                           -554.608005             \n",
       "713.0                                             11.621645             \n",
       "730.0                                             -3.167655             \n",
       "745.0                                             50.504436             \n",
       "1124.0                                           -18.002683             \n",
       "\n",
       "variable  b3__c3__lag_2  b5__symmetry_looking__r_0.25  \\\n",
       "id                                                      \n",
       "615.0      1.525137e+07                           1.0   \n",
       "713.0     -8.940954e+01                           1.0   \n",
       "730.0      2.138440e+02                           1.0   \n",
       "745.0      6.531625e+03                           1.0   \n",
       "1124.0     1.037487e+04                           1.0   \n",
       "\n",
       "variable  b5__symmetry_looking__r_0.30000000000000004  \\\n",
       "id                                                      \n",
       "615.0                                             1.0   \n",
       "713.0                                             1.0   \n",
       "730.0                                             1.0   \n",
       "745.0                                             1.0   \n",
       "1124.0                                            1.0   \n",
       "\n",
       "variable  b1__symmetry_looking__r_0.30000000000000004  \n",
       "id                                                     \n",
       "615.0                                             1.0  \n",
       "713.0                                             1.0  \n",
       "730.0                                             1.0  \n",
       "745.0                                             1.0  \n",
       "1124.0                                            1.0  \n",
       "\n",
       "[5 rows x 1902 columns]"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfbands = pd.read_pickle('X_filtered.pkl')\n",
    "# bands = pd.read_pickle('df_transformed.pkl')\n",
    "dfbands.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "bs = ['b1','b2','b3','b4','b5','b6']\n",
    "remaining_features = ['approximate_entropy', 'autocorrelation', 'c3', 'change_quantiles', #'absolute_sum_of_changes'\n",
    "                     'cid_ce', 'count_above_mean', 'count_below_mean', 'energy_ratio_by_chunks', \n",
    "                     'fft_aggregated', 'fft_coefficient', #'last_location_of_maximum',   #\n",
    "                     'longest_strike_above_mean', 'mean_abs_change', 'range_count', 'skewness'] #'quantile' or relative change in quantile?\n",
    "listf = [] \n",
    "for i in bands.columns:\n",
    "    for f in remaining_features:\n",
    "        if f in i:\n",
    "            listf.append(i)\n",
    "dfbands_reduced = dfbands[listf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'target'"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfMetaData = pd.read_csv(\"training_set_metadata.csv\")\n",
    "dfMerged = dfbands_reduced.merge(dfMetaData,left_on = \"id\",right_on = \"object_id\")\n",
    "dfMerged = dfMerged.drop([\"distmod\",\"object_id\"],axis = 1)\n",
    "dfMerged.columns[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance(df):     \n",
    "    return resample(df,\n",
    "                    replace=True, \n",
    "                    n_samples=2313,   #max target 90\n",
    "                    random_state=123)\n",
    "\n",
    "def bootstrap(df,cntindex):\n",
    "    idcounts = df.iloc[:, cntindex].value_counts().to_frame('counts')\n",
    "#     print(idcounts)\n",
    "    df90 = df.loc[df.iloc[:,cntindex]==90]\n",
    "    minority_class = [42, 65, 16, 15, 62, 88, 92, 67, 52, 95, 6, 64, 53]\n",
    "    upsampled_list = [df90]  \n",
    "\n",
    "    for m in minority_class:\n",
    "        t = df.loc[df.iloc[:,cntindex]==m]\n",
    "        upsampled_list.append(balance(t))\n",
    "    df_upsampled = pd.concat([i for i in upsampled_list])\n",
    "    \n",
    "    return df_upsampled\n",
    " \n",
    "dfMerged = bootstrap(dfMerged, -1)     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32382, 855)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfMerged.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_hidden_neurons = 128\n",
    "dropout = 0.25\n",
    "epochs = 200\n",
    "\n",
    "#preprocessing \n",
    "dfTrain, dfTest = train_test_split(dfMerged)\n",
    "\n",
    "y_train = dfTrain[\"target\"]\n",
    "X_train = dfTrain.drop([\"target\"],axis = 1)\n",
    "\n",
    "y_test = dfTest[\"target\"]\n",
    "X_test = dfTest.drop([\"target\"],axis = 1)\n",
    "\n",
    "normalizer = Normalizer()\n",
    "X_train = normalizer.fit_transform(X_train)\n",
    "X_test = normalizer.transform(X_test)\n",
    "\n",
    "#convert y to categorical \n",
    "unique_y = np.unique(y_train)\n",
    "class_map = dict()\n",
    "for i,val in enumerate(unique_y):\n",
    "    class_map[val] = i\n",
    "        \n",
    "y_map = np.zeros((y_train.shape[0],))\n",
    "y_map = np.array([class_map[val] for val in y_train])\n",
    "y_categorical = to_categorical(y_map)\n",
    "\n",
    "unique_y_test = np.unique(y_test)\n",
    "class_map_test = dict()\n",
    "for i,val in enumerate(unique_y_test):\n",
    "    class_map_test[val] = i\n",
    "        \n",
    "y_map_test = np.zeros((y_test.shape[0],))\n",
    "y_map_test = np.array([class_map_test[val] for val in y_test])\n",
    "y_categorical_test = to_categorical(y_map_test)\n",
    "\n",
    "#loss function\n",
    "y_count = collections.Counter(y_map)\n",
    "wtable = np.zeros((len(unique_y),))\n",
    "\n",
    "for i in range(len(unique_y)):\n",
    "    wtable[i] = y_count[i]/y_map.shape[0]\n",
    "    \n",
    "def mywloss(y_true,y_pred):  \n",
    "    yc=tf.clip_by_value(y_pred,1e-15,1-1e-15)\n",
    "    loss=-(tf.reduce_mean(tf.reduce_mean(y_true*tf.log(yc),axis=0)/wtable))\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24286 samples, validate on 8096 samples\n",
      "Epoch 1/200\n",
      "24286/24286 [==============================] - 2s 91us/step - loss: 1.8720 - acc: 0.3837 - val_loss: 1.5339 - val_acc: 0.4967\n",
      "Epoch 2/200\n",
      "24286/24286 [==============================] - 1s 58us/step - loss: 1.4932 - acc: 0.5016 - val_loss: 1.3512 - val_acc: 0.5564\n",
      "Epoch 3/200\n",
      "24286/24286 [==============================] - 1s 58us/step - loss: 1.3460 - acc: 0.5523 - val_loss: 1.2382 - val_acc: 0.6005\n",
      "Epoch 4/200\n",
      "24286/24286 [==============================] - 1s 59us/step - loss: 1.2412 - acc: 0.5908 - val_loss: 1.1454 - val_acc: 0.6248\n",
      "Epoch 5/200\n",
      "24286/24286 [==============================] - 1s 59us/step - loss: 1.1545 - acc: 0.6201 - val_loss: 1.0617 - val_acc: 0.6682\n",
      "Epoch 6/200\n",
      "24286/24286 [==============================] - 1s 61us/step - loss: 1.0941 - acc: 0.6381 - val_loss: 1.0009 - val_acc: 0.6785\n",
      "Epoch 7/200\n",
      "24286/24286 [==============================] - 1s 61us/step - loss: 1.0311 - acc: 0.6604 - val_loss: 0.9398 - val_acc: 0.7010\n",
      "Epoch 8/200\n",
      "24286/24286 [==============================] - 2s 65us/step - loss: 0.9891 - acc: 0.6738 - val_loss: 0.8964 - val_acc: 0.7174\n",
      "Epoch 9/200\n",
      "24286/24286 [==============================] - 2s 65us/step - loss: 0.9403 - acc: 0.6908 - val_loss: 0.8552 - val_acc: 0.7279\n",
      "Epoch 10/200\n",
      "24286/24286 [==============================] - 2s 71us/step - loss: 0.9017 - acc: 0.7037 - val_loss: 0.8220 - val_acc: 0.7354\n",
      "Epoch 11/200\n",
      "24286/24286 [==============================] - 2s 65us/step - loss: 0.8759 - acc: 0.7078 - val_loss: 0.7954 - val_acc: 0.7470\n",
      "Epoch 12/200\n",
      "24286/24286 [==============================] - 2s 66us/step - loss: 0.8439 - acc: 0.7170 - val_loss: 0.7651 - val_acc: 0.7568\n",
      "Epoch 13/200\n",
      "24286/24286 [==============================] - 2s 65us/step - loss: 0.8167 - acc: 0.7260 - val_loss: 0.7390 - val_acc: 0.7626\n",
      "Epoch 14/200\n",
      "24286/24286 [==============================] - 1s 60us/step - loss: 0.7882 - acc: 0.7401 - val_loss: 0.7169 - val_acc: 0.7684\n",
      "Epoch 15/200\n",
      "24286/24286 [==============================] - 1s 61us/step - loss: 0.7657 - acc: 0.7450 - val_loss: 0.6981 - val_acc: 0.7742\n",
      "Epoch 16/200\n",
      "24286/24286 [==============================] - 1s 60us/step - loss: 0.7446 - acc: 0.7524 - val_loss: 0.6749 - val_acc: 0.7863\n",
      "Epoch 17/200\n",
      "24286/24286 [==============================] - 1s 62us/step - loss: 0.7241 - acc: 0.7597 - val_loss: 0.6508 - val_acc: 0.7925\n",
      "Epoch 18/200\n",
      "24286/24286 [==============================] - 2s 64us/step - loss: 0.7111 - acc: 0.7604 - val_loss: 0.6367 - val_acc: 0.8010\n",
      "Epoch 19/200\n",
      "24286/24286 [==============================] - 2s 71us/step - loss: 0.6953 - acc: 0.7675 - val_loss: 0.6276 - val_acc: 0.8087\n",
      "Epoch 20/200\n",
      "24286/24286 [==============================] - 2s 65us/step - loss: 0.6755 - acc: 0.7716 - val_loss: 0.6156 - val_acc: 0.8081\n",
      "Epoch 21/200\n",
      "24286/24286 [==============================] - 2s 63us/step - loss: 0.6615 - acc: 0.7781 - val_loss: 0.6038 - val_acc: 0.8127\n",
      "Epoch 22/200\n",
      "24286/24286 [==============================] - 2s 66us/step - loss: 0.6537 - acc: 0.7816 - val_loss: 0.5980 - val_acc: 0.8153\n",
      "Epoch 23/200\n",
      "24286/24286 [==============================] - 2s 72us/step - loss: 0.6412 - acc: 0.7830 - val_loss: 0.5857 - val_acc: 0.8255\n",
      "Epoch 24/200\n",
      "24286/24286 [==============================] - 2s 71us/step - loss: 0.6316 - acc: 0.7867 - val_loss: 0.5726 - val_acc: 0.8221\n",
      "Epoch 25/200\n",
      "24286/24286 [==============================] - 2s 63us/step - loss: 0.6268 - acc: 0.7899 - val_loss: 0.5650 - val_acc: 0.8231\n",
      "Epoch 26/200\n",
      "24286/24286 [==============================] - 2s 65us/step - loss: 0.6114 - acc: 0.7982 - val_loss: 0.5555 - val_acc: 0.8299\n",
      "Epoch 27/200\n",
      "24286/24286 [==============================] - 1s 60us/step - loss: 0.6006 - acc: 0.7970 - val_loss: 0.5534 - val_acc: 0.8276\n",
      "Epoch 28/200\n",
      "24286/24286 [==============================] - 1s 61us/step - loss: 0.5900 - acc: 0.8058 - val_loss: 0.5427 - val_acc: 0.8312\n",
      "Epoch 29/200\n",
      "24286/24286 [==============================] - 2s 65us/step - loss: 0.5809 - acc: 0.8056 - val_loss: 0.5232 - val_acc: 0.8379\n",
      "Epoch 30/200\n",
      "24286/24286 [==============================] - 2s 64us/step - loss: 0.5766 - acc: 0.8084 - val_loss: 0.5244 - val_acc: 0.8455\n",
      "Epoch 31/200\n",
      "24286/24286 [==============================] - 1s 61us/step - loss: 0.5691 - acc: 0.8120 - val_loss: 0.5235 - val_acc: 0.8389\n",
      "Epoch 32/200\n",
      "24286/24286 [==============================] - 1s 60us/step - loss: 0.5571 - acc: 0.8132 - val_loss: 0.5130 - val_acc: 0.8378\n",
      "Epoch 33/200\n",
      "24286/24286 [==============================] - 1s 60us/step - loss: 0.5526 - acc: 0.8160 - val_loss: 0.5091 - val_acc: 0.8463\n",
      "Epoch 34/200\n",
      "24286/24286 [==============================] - 1s 59us/step - loss: 0.5482 - acc: 0.8156 - val_loss: 0.5109 - val_acc: 0.8433\n",
      "Epoch 35/200\n",
      "24286/24286 [==============================] - 2s 65us/step - loss: 0.5378 - acc: 0.8192 - val_loss: 0.4978 - val_acc: 0.8478\n",
      "Epoch 36/200\n",
      "24286/24286 [==============================] - 2s 63us/step - loss: 0.5332 - acc: 0.8199 - val_loss: 0.4853 - val_acc: 0.8500\n",
      "Epoch 37/200\n",
      "24286/24286 [==============================] - 1s 59us/step - loss: 0.5267 - acc: 0.8219 - val_loss: 0.4832 - val_acc: 0.8539\n",
      "Epoch 38/200\n",
      "24286/24286 [==============================] - 1s 60us/step - loss: 0.5228 - acc: 0.8243 - val_loss: 0.4788 - val_acc: 0.8531\n",
      "Epoch 39/200\n",
      "24286/24286 [==============================] - 1s 59us/step - loss: 0.5136 - acc: 0.8275 - val_loss: 0.4850 - val_acc: 0.8492\n",
      "Epoch 40/200\n",
      "24286/24286 [==============================] - 1s 60us/step - loss: 0.5148 - acc: 0.8240 - val_loss: 0.4868 - val_acc: 0.8477\n",
      "Epoch 41/200\n",
      "24286/24286 [==============================] - 2s 63us/step - loss: 0.5095 - acc: 0.8288 - val_loss: 0.4628 - val_acc: 0.8589\n",
      "Epoch 42/200\n",
      "24286/24286 [==============================] - 1s 58us/step - loss: 0.5022 - acc: 0.8304 - val_loss: 0.4779 - val_acc: 0.8530\n",
      "Epoch 43/200\n",
      "24286/24286 [==============================] - 1s 61us/step - loss: 0.4929 - acc: 0.8354 - val_loss: 0.4618 - val_acc: 0.8560\n",
      "Epoch 44/200\n",
      "24286/24286 [==============================] - 2s 62us/step - loss: 0.4894 - acc: 0.8371 - val_loss: 0.4569 - val_acc: 0.8633\n",
      "Epoch 45/200\n",
      "24286/24286 [==============================] - 1s 61us/step - loss: 0.4887 - acc: 0.8357 - val_loss: 0.4543 - val_acc: 0.8591\n",
      "Epoch 46/200\n",
      "24286/24286 [==============================] - 1s 59us/step - loss: 0.4838 - acc: 0.8366 - val_loss: 0.4588 - val_acc: 0.8587\n",
      "Epoch 47/200\n",
      "24286/24286 [==============================] - 2s 64us/step - loss: 0.4763 - acc: 0.8374 - val_loss: 0.4570 - val_acc: 0.8609\n",
      "Epoch 48/200\n",
      "24286/24286 [==============================] - 2s 62us/step - loss: 0.4741 - acc: 0.8434 - val_loss: 0.4457 - val_acc: 0.8624\n",
      "Epoch 49/200\n",
      "24286/24286 [==============================] - 2s 62us/step - loss: 0.4751 - acc: 0.8390 - val_loss: 0.4571 - val_acc: 0.8645\n",
      "Epoch 50/200\n",
      "24286/24286 [==============================] - 1s 59us/step - loss: 0.4649 - acc: 0.8425 - val_loss: 0.4488 - val_acc: 0.8675\n",
      "Epoch 51/200\n",
      "24286/24286 [==============================] - 2s 65us/step - loss: 0.4646 - acc: 0.8443 - val_loss: 0.4482 - val_acc: 0.8624\n",
      "Epoch 52/200\n",
      "24286/24286 [==============================] - 2s 67us/step - loss: 0.4634 - acc: 0.8449 - val_loss: 0.4447 - val_acc: 0.8664\n",
      "Epoch 53/200\n",
      "24286/24286 [==============================] - 2s 70us/step - loss: 0.4554 - acc: 0.8472 - val_loss: 0.4431 - val_acc: 0.8636\n",
      "Epoch 54/200\n",
      "24286/24286 [==============================] - 2s 72us/step - loss: 0.4510 - acc: 0.8460 - val_loss: 0.4357 - val_acc: 0.8689\n",
      "Epoch 55/200\n",
      "24286/24286 [==============================] - 2s 73us/step - loss: 0.4510 - acc: 0.8481 - val_loss: 0.4389 - val_acc: 0.8683\n",
      "Epoch 56/200\n",
      "24286/24286 [==============================] - 2s 67us/step - loss: 0.4486 - acc: 0.8488 - val_loss: 0.4407 - val_acc: 0.8656\n",
      "Epoch 57/200\n",
      "24286/24286 [==============================] - 2s 73us/step - loss: 0.4415 - acc: 0.8523 - val_loss: 0.4325 - val_acc: 0.8709\n",
      "Epoch 58/200\n",
      "24286/24286 [==============================] - 2s 72us/step - loss: 0.4434 - acc: 0.8492 - val_loss: 0.4316 - val_acc: 0.8696\n",
      "Epoch 59/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24286/24286 [==============================] - 1s 58us/step - loss: 0.4369 - acc: 0.8547 - val_loss: 0.4354 - val_acc: 0.8693\n",
      "Epoch 60/200\n",
      "24286/24286 [==============================] - 2s 63us/step - loss: 0.4323 - acc: 0.8535 - val_loss: 0.4185 - val_acc: 0.8746\n",
      "Epoch 61/200\n",
      "24286/24286 [==============================] - 1s 59us/step - loss: 0.4284 - acc: 0.8552 - val_loss: 0.4316 - val_acc: 0.8702\n",
      "Epoch 62/200\n",
      "24286/24286 [==============================] - 2s 65us/step - loss: 0.4301 - acc: 0.8535 - val_loss: 0.4164 - val_acc: 0.8756\n",
      "Epoch 63/200\n",
      "24286/24286 [==============================] - 2s 65us/step - loss: 0.4248 - acc: 0.8556 - val_loss: 0.4199 - val_acc: 0.8773\n",
      "Epoch 64/200\n",
      "24286/24286 [==============================] - 1s 60us/step - loss: 0.4284 - acc: 0.8538 - val_loss: 0.4181 - val_acc: 0.8715\n",
      "Epoch 65/200\n",
      "24286/24286 [==============================] - 2s 68us/step - loss: 0.4175 - acc: 0.8578 - val_loss: 0.4104 - val_acc: 0.8807\n",
      "Epoch 66/200\n",
      "24286/24286 [==============================] - 2s 67us/step - loss: 0.4185 - acc: 0.8579 - val_loss: 0.4210 - val_acc: 0.8759\n",
      "Epoch 67/200\n",
      "24286/24286 [==============================] - 2s 93us/step - loss: 0.4198 - acc: 0.8584 - val_loss: 0.4185 - val_acc: 0.8722\n",
      "Epoch 68/200\n",
      "24286/24286 [==============================] - 2s 67us/step - loss: 0.4209 - acc: 0.8569 - val_loss: 0.4101 - val_acc: 0.8817\n",
      "Epoch 69/200\n",
      "24286/24286 [==============================] - 2s 71us/step - loss: 0.4135 - acc: 0.8578 - val_loss: 0.4154 - val_acc: 0.8735\n",
      "Epoch 70/200\n",
      "24286/24286 [==============================] - 2s 67us/step - loss: 0.4150 - acc: 0.8575 - val_loss: 0.4209 - val_acc: 0.8785\n",
      "Epoch 71/200\n",
      "24286/24286 [==============================] - 2s 69us/step - loss: 0.4126 - acc: 0.8591 - val_loss: 0.4042 - val_acc: 0.8813\n",
      "Epoch 72/200\n",
      "24286/24286 [==============================] - 2s 67us/step - loss: 0.4086 - acc: 0.8642 - val_loss: 0.4058 - val_acc: 0.8850\n",
      "Epoch 73/200\n",
      "24286/24286 [==============================] - 2s 68us/step - loss: 0.4043 - acc: 0.8630 - val_loss: 0.3997 - val_acc: 0.8866\n",
      "Epoch 74/200\n",
      "24286/24286 [==============================] - 2s 75us/step - loss: 0.4075 - acc: 0.8616 - val_loss: 0.4011 - val_acc: 0.8824\n",
      "Epoch 75/200\n",
      "24286/24286 [==============================] - 2s 70us/step - loss: 0.3989 - acc: 0.8642 - val_loss: 0.4067 - val_acc: 0.8820\n",
      "Epoch 76/200\n",
      "24286/24286 [==============================] - 2s 71us/step - loss: 0.3883 - acc: 0.8700 - val_loss: 0.4187 - val_acc: 0.8827\n",
      "Epoch 77/200\n",
      "24286/24286 [==============================] - 2s 68us/step - loss: 0.3950 - acc: 0.8678 - val_loss: 0.4022 - val_acc: 0.8853\n",
      "Epoch 78/200\n",
      "24286/24286 [==============================] - 2s 72us/step - loss: 0.3947 - acc: 0.8661 - val_loss: 0.4156 - val_acc: 0.8746\n",
      "Epoch 79/200\n",
      "24286/24286 [==============================] - 2s 74us/step - loss: 0.3921 - acc: 0.8676 - val_loss: 0.3933 - val_acc: 0.8855\n",
      "Epoch 80/200\n",
      "24286/24286 [==============================] - 2s 73us/step - loss: 0.3878 - acc: 0.8682 - val_loss: 0.3999 - val_acc: 0.8860\n",
      "Epoch 81/200\n",
      "24286/24286 [==============================] - 2s 71us/step - loss: 0.3833 - acc: 0.8715 - val_loss: 0.3905 - val_acc: 0.8922\n",
      "Epoch 82/200\n",
      "24286/24286 [==============================] - 2s 75us/step - loss: 0.3834 - acc: 0.8716 - val_loss: 0.4015 - val_acc: 0.8878\n",
      "Epoch 83/200\n",
      "24286/24286 [==============================] - 2s 68us/step - loss: 0.3812 - acc: 0.8687 - val_loss: 0.3969 - val_acc: 0.8890\n",
      "Epoch 84/200\n",
      "24286/24286 [==============================] - 2s 68us/step - loss: 0.3860 - acc: 0.8672 - val_loss: 0.3871 - val_acc: 0.8885\n",
      "Epoch 85/200\n",
      "24286/24286 [==============================] - 2s 80us/step - loss: 0.3780 - acc: 0.8726 - val_loss: 0.3874 - val_acc: 0.8933\n",
      "Epoch 86/200\n",
      "24286/24286 [==============================] - 2s 71us/step - loss: 0.3797 - acc: 0.8713 - val_loss: 0.3939 - val_acc: 0.8878\n",
      "Epoch 87/200\n",
      "24286/24286 [==============================] - 2s 84us/step - loss: 0.3779 - acc: 0.8722 - val_loss: 0.3953 - val_acc: 0.8876\n",
      "Epoch 88/200\n",
      "24286/24286 [==============================] - 2s 71us/step - loss: 0.3746 - acc: 0.8712 - val_loss: 0.3852 - val_acc: 0.8895\n",
      "Epoch 89/200\n",
      "24286/24286 [==============================] - 2s 74us/step - loss: 0.3715 - acc: 0.8720 - val_loss: 0.3938 - val_acc: 0.8874\n",
      "Epoch 90/200\n",
      "24286/24286 [==============================] - 2s 72us/step - loss: 0.3676 - acc: 0.8735 - val_loss: 0.3870 - val_acc: 0.8891\n",
      "Epoch 91/200\n",
      "24286/24286 [==============================] - 2s 65us/step - loss: 0.3785 - acc: 0.8737 - val_loss: 0.3819 - val_acc: 0.8935\n",
      "Epoch 92/200\n",
      "24286/24286 [==============================] - 2s 65us/step - loss: 0.3674 - acc: 0.8756 - val_loss: 0.3968 - val_acc: 0.8832\n",
      "Epoch 93/200\n",
      "24286/24286 [==============================] - 2s 66us/step - loss: 0.3634 - acc: 0.8776 - val_loss: 0.3818 - val_acc: 0.8934\n",
      "Epoch 94/200\n",
      "24286/24286 [==============================] - 2s 68us/step - loss: 0.3683 - acc: 0.8731 - val_loss: 0.3850 - val_acc: 0.8913\n",
      "Epoch 95/200\n",
      "24286/24286 [==============================] - 2s 67us/step - loss: 0.3618 - acc: 0.8783 - val_loss: 0.3882 - val_acc: 0.8908\n",
      "Epoch 96/200\n",
      "24286/24286 [==============================] - 2s 65us/step - loss: 0.3696 - acc: 0.8740 - val_loss: 0.3852 - val_acc: 0.8890\n",
      "Epoch 97/200\n",
      "24286/24286 [==============================] - 2s 67us/step - loss: 0.3595 - acc: 0.8772 - val_loss: 0.3882 - val_acc: 0.8920\n",
      "Epoch 98/200\n",
      "24286/24286 [==============================] - 2s 68us/step - loss: 0.3595 - acc: 0.8768 - val_loss: 0.3793 - val_acc: 0.8906\n",
      "Epoch 99/200\n",
      "24286/24286 [==============================] - 2s 70us/step - loss: 0.3557 - acc: 0.8781 - val_loss: 0.3786 - val_acc: 0.8932\n",
      "Epoch 100/200\n",
      "24286/24286 [==============================] - 2s 66us/step - loss: 0.3539 - acc: 0.8808 - val_loss: 0.3840 - val_acc: 0.8897\n",
      "Epoch 101/200\n",
      "24286/24286 [==============================] - 2s 66us/step - loss: 0.3617 - acc: 0.8760 - val_loss: 0.3842 - val_acc: 0.8938\n",
      "Epoch 102/200\n",
      "24286/24286 [==============================] - 2s 65us/step - loss: 0.3629 - acc: 0.8773 - val_loss: 0.3840 - val_acc: 0.8954\n",
      "Epoch 103/200\n",
      "24286/24286 [==============================] - 2s 65us/step - loss: 0.3453 - acc: 0.8819 - val_loss: 0.3863 - val_acc: 0.8934\n",
      "Epoch 104/200\n",
      "24286/24286 [==============================] - 2s 65us/step - loss: 0.3486 - acc: 0.8812 - val_loss: 0.3722 - val_acc: 0.8939\n",
      "Epoch 105/200\n",
      "24286/24286 [==============================] - 2s 68us/step - loss: 0.3561 - acc: 0.8782 - val_loss: 0.3766 - val_acc: 0.8934\n",
      "Epoch 106/200\n",
      "24286/24286 [==============================] - 2s 65us/step - loss: 0.3385 - acc: 0.8832 - val_loss: 0.3807 - val_acc: 0.8927\n",
      "Epoch 107/200\n",
      "24286/24286 [==============================] - 2s 66us/step - loss: 0.3353 - acc: 0.8871 - val_loss: 0.3788 - val_acc: 0.8965\n",
      "Epoch 108/200\n",
      "24286/24286 [==============================] - 2s 65us/step - loss: 0.3449 - acc: 0.8820 - val_loss: 0.3691 - val_acc: 0.8991\n",
      "Epoch 109/200\n",
      "24286/24286 [==============================] - 2s 66us/step - loss: 0.3475 - acc: 0.8809 - val_loss: 0.3659 - val_acc: 0.8987\n",
      "Epoch 110/200\n",
      "24286/24286 [==============================] - 2s 67us/step - loss: 0.3511 - acc: 0.8793 - val_loss: 0.3799 - val_acc: 0.8960\n",
      "Epoch 111/200\n",
      "24286/24286 [==============================] - 2s 65us/step - loss: 0.3495 - acc: 0.8826 - val_loss: 0.3684 - val_acc: 0.9021\n",
      "Epoch 112/200\n",
      "24286/24286 [==============================] - 2s 65us/step - loss: 0.3439 - acc: 0.8825 - val_loss: 0.3678 - val_acc: 0.9032\n",
      "Epoch 113/200\n",
      "24286/24286 [==============================] - 2s 65us/step - loss: 0.3501 - acc: 0.8811 - val_loss: 0.3754 - val_acc: 0.8951\n",
      "Epoch 114/200\n",
      "24286/24286 [==============================] - 2s 65us/step - loss: 0.3418 - acc: 0.8842 - val_loss: 0.3801 - val_acc: 0.8962\n",
      "Epoch 115/200\n",
      "24286/24286 [==============================] - 2s 67us/step - loss: 0.3383 - acc: 0.8869 - val_loss: 0.3679 - val_acc: 0.8972\n",
      "Epoch 116/200\n",
      "24286/24286 [==============================] - 2s 66us/step - loss: 0.3364 - acc: 0.8861 - val_loss: 0.3761 - val_acc: 0.8988\n",
      "Epoch 117/200\n",
      "24286/24286 [==============================] - 2s 67us/step - loss: 0.3369 - acc: 0.8850 - val_loss: 0.3691 - val_acc: 0.8982\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/200\n",
      "24286/24286 [==============================] - 1s 60us/step - loss: 0.3322 - acc: 0.8851 - val_loss: 0.3831 - val_acc: 0.8972\n",
      "Epoch 119/200\n",
      "24286/24286 [==============================] - 1s 56us/step - loss: 0.3341 - acc: 0.8832 - val_loss: 0.3806 - val_acc: 0.8967\n",
      "Epoch 120/200\n",
      "24286/24286 [==============================] - 1s 57us/step - loss: 0.3302 - acc: 0.8889 - val_loss: 0.3854 - val_acc: 0.8962\n",
      "Epoch 121/200\n",
      "24286/24286 [==============================] - 1s 57us/step - loss: 0.3312 - acc: 0.8864 - val_loss: 0.3942 - val_acc: 0.8917\n",
      "Epoch 122/200\n",
      "24286/24286 [==============================] - 1s 57us/step - loss: 0.3304 - acc: 0.8871 - val_loss: 0.3687 - val_acc: 0.9019\n",
      "Epoch 123/200\n",
      "24286/24286 [==============================] - 1s 57us/step - loss: 0.3304 - acc: 0.8873 - val_loss: 0.3907 - val_acc: 0.8951\n",
      "Epoch 124/200\n",
      "24286/24286 [==============================] - 2s 68us/step - loss: 0.3265 - acc: 0.8913 - val_loss: 0.3803 - val_acc: 0.8990\n",
      "Epoch 125/200\n",
      "24286/24286 [==============================] - 2s 67us/step - loss: 0.3194 - acc: 0.8920 - val_loss: 0.3761 - val_acc: 0.8997\n",
      "Epoch 126/200\n",
      "24286/24286 [==============================] - 2s 68us/step - loss: 0.3231 - acc: 0.8894 - val_loss: 0.3546 - val_acc: 0.9077\n",
      "Epoch 127/200\n",
      "24286/24286 [==============================] - 2s 67us/step - loss: 0.3177 - acc: 0.8890 - val_loss: 0.3582 - val_acc: 0.9058\n",
      "Epoch 128/200\n",
      "24286/24286 [==============================] - 2s 75us/step - loss: 0.3200 - acc: 0.8910 - val_loss: 0.3707 - val_acc: 0.9017\n",
      "Epoch 129/200\n",
      "24286/24286 [==============================] - 2s 74us/step - loss: 0.3261 - acc: 0.8862 - val_loss: 0.3720 - val_acc: 0.9001\n",
      "Epoch 130/200\n",
      "24286/24286 [==============================] - 2s 68us/step - loss: 0.3219 - acc: 0.8906 - val_loss: 0.3668 - val_acc: 0.9043\n",
      "Epoch 131/200\n",
      "24286/24286 [==============================] - 1s 62us/step - loss: 0.3246 - acc: 0.8890 - val_loss: 0.3616 - val_acc: 0.9044\n",
      "Epoch 132/200\n",
      "24286/24286 [==============================] - 1s 58us/step - loss: 0.3179 - acc: 0.8906 - val_loss: 0.3771 - val_acc: 0.9001\n",
      "Epoch 133/200\n",
      "24286/24286 [==============================] - 1s 57us/step - loss: 0.3228 - acc: 0.8915 - val_loss: 0.3684 - val_acc: 0.9028\n",
      "Epoch 134/200\n",
      "24286/24286 [==============================] - 1s 59us/step - loss: 0.3249 - acc: 0.8895 - val_loss: 0.3751 - val_acc: 0.8956\n",
      "Epoch 135/200\n",
      "24286/24286 [==============================] - 2s 77us/step - loss: 0.3171 - acc: 0.8928 - val_loss: 0.3754 - val_acc: 0.9035\n",
      "Epoch 136/200\n",
      "24286/24286 [==============================] - 1s 59us/step - loss: 0.3170 - acc: 0.8933 - val_loss: 0.3657 - val_acc: 0.9064\n",
      "Epoch 137/200\n",
      "24286/24286 [==============================] - 2s 68us/step - loss: 0.3154 - acc: 0.8930 - val_loss: 0.3659 - val_acc: 0.9054\n",
      "Epoch 138/200\n",
      "24286/24286 [==============================] - 1s 59us/step - loss: 0.3239 - acc: 0.8890 - val_loss: 0.3728 - val_acc: 0.9034\n",
      "Epoch 139/200\n",
      "24286/24286 [==============================] - 1s 57us/step - loss: 0.3143 - acc: 0.8933 - val_loss: 0.3657 - val_acc: 0.9016\n",
      "Epoch 140/200\n",
      "24286/24286 [==============================] - 1s 59us/step - loss: 0.3163 - acc: 0.8906 - val_loss: 0.3684 - val_acc: 0.9038\n",
      "Epoch 141/200\n",
      "24286/24286 [==============================] - 1s 57us/step - loss: 0.3129 - acc: 0.8920 - val_loss: 0.3648 - val_acc: 0.9083\n",
      "Epoch 142/200\n",
      "24286/24286 [==============================] - 2s 67us/step - loss: 0.3126 - acc: 0.8922 - val_loss: 0.3675 - val_acc: 0.9060\n",
      "Epoch 143/200\n",
      "24286/24286 [==============================] - 2s 71us/step - loss: 0.3076 - acc: 0.8959 - val_loss: 0.3630 - val_acc: 0.9030\n",
      "Epoch 144/200\n",
      "24286/24286 [==============================] - 2s 68us/step - loss: 0.3195 - acc: 0.8907 - val_loss: 0.3607 - val_acc: 0.9024\n",
      "Epoch 145/200\n",
      "24286/24286 [==============================] - 2s 78us/step - loss: 0.3127 - acc: 0.8941 - val_loss: 0.3732 - val_acc: 0.8992\n",
      "Epoch 146/200\n",
      "24286/24286 [==============================] - 2s 62us/step - loss: 0.3119 - acc: 0.8920 - val_loss: 0.3593 - val_acc: 0.9077\n",
      "Epoch 147/200\n",
      "24286/24286 [==============================] - 2s 63us/step - loss: 0.3105 - acc: 0.8939 - val_loss: 0.3667 - val_acc: 0.9008\n",
      "Epoch 148/200\n",
      "24286/24286 [==============================] - 2s 63us/step - loss: 0.3062 - acc: 0.8970 - val_loss: 0.3618 - val_acc: 0.9075\n",
      "Epoch 149/200\n",
      "24286/24286 [==============================] - 2s 67us/step - loss: 0.3061 - acc: 0.8949 - val_loss: 0.3610 - val_acc: 0.9049\n",
      "Epoch 150/200\n",
      "24286/24286 [==============================] - 1s 62us/step - loss: 0.3028 - acc: 0.8957 - val_loss: 0.3883 - val_acc: 0.8993\n",
      "Epoch 151/200\n",
      "24286/24286 [==============================] - 2s 70us/step - loss: 0.3067 - acc: 0.8968 - val_loss: 0.3727 - val_acc: 0.9053\n",
      "Epoch 152/200\n",
      "24286/24286 [==============================] - 2s 65us/step - loss: 0.3111 - acc: 0.8943 - val_loss: 0.3785 - val_acc: 0.9038\n",
      "Epoch 153/200\n",
      "24286/24286 [==============================] - 2s 62us/step - loss: 0.3016 - acc: 0.8969 - val_loss: 0.3576 - val_acc: 0.9133\n",
      "Epoch 154/200\n",
      "24286/24286 [==============================] - 2s 67us/step - loss: 0.3066 - acc: 0.8952 - val_loss: 0.3741 - val_acc: 0.9054\n",
      "Epoch 155/200\n",
      "24286/24286 [==============================] - 2s 87us/step - loss: 0.3075 - acc: 0.8952 - val_loss: 0.3877 - val_acc: 0.9003\n",
      "Epoch 156/200\n",
      "24286/24286 [==============================] - 2s 75us/step - loss: 0.2991 - acc: 0.8972 - val_loss: 0.3748 - val_acc: 0.9066\n",
      "Epoch 157/200\n",
      "24286/24286 [==============================] - 2s 76us/step - loss: 0.3043 - acc: 0.8966 - val_loss: 0.3732 - val_acc: 0.9066\n",
      "Epoch 158/200\n",
      "24286/24286 [==============================] - 2s 80us/step - loss: 0.3010 - acc: 0.8981 - val_loss: 0.3634 - val_acc: 0.9065\n",
      "Epoch 159/200\n",
      "24286/24286 [==============================] - 2s 73us/step - loss: 0.2998 - acc: 0.8986 - val_loss: 0.3626 - val_acc: 0.9087\n",
      "Epoch 160/200\n",
      "24286/24286 [==============================] - 2s 66us/step - loss: 0.2975 - acc: 0.8981 - val_loss: 0.3782 - val_acc: 0.9001\n",
      "Epoch 161/200\n",
      "24286/24286 [==============================] - 2s 70us/step - loss: 0.2978 - acc: 0.8990 - val_loss: 0.3666 - val_acc: 0.9055\n",
      "Epoch 162/200\n",
      "24286/24286 [==============================] - 2s 71us/step - loss: 0.3000 - acc: 0.8980 - val_loss: 0.3697 - val_acc: 0.9023\n",
      "Epoch 163/200\n",
      "24286/24286 [==============================] - 2s 72us/step - loss: 0.2920 - acc: 0.8990 - val_loss: 0.3589 - val_acc: 0.9071\n",
      "Epoch 164/200\n",
      "24286/24286 [==============================] - 2s 72us/step - loss: 0.3012 - acc: 0.8979 - val_loss: 0.3719 - val_acc: 0.9037\n",
      "Epoch 165/200\n",
      "24286/24286 [==============================] - 2s 69us/step - loss: 0.3015 - acc: 0.8971 - val_loss: 0.3653 - val_acc: 0.9066\n",
      "Epoch 166/200\n",
      "24286/24286 [==============================] - 2s 66us/step - loss: 0.2931 - acc: 0.9005 - val_loss: 0.3681 - val_acc: 0.9039\n",
      "Epoch 167/200\n",
      "24286/24286 [==============================] - 2s 73us/step - loss: 0.3035 - acc: 0.8950 - val_loss: 0.3579 - val_acc: 0.9153\n",
      "Epoch 168/200\n",
      "24286/24286 [==============================] - 2s 73us/step - loss: 0.2926 - acc: 0.9004 - val_loss: 0.3673 - val_acc: 0.9038\n",
      "Epoch 169/200\n",
      "24286/24286 [==============================] - 2s 70us/step - loss: 0.2955 - acc: 0.8983 - val_loss: 0.3684 - val_acc: 0.9045\n",
      "Epoch 170/200\n",
      "24286/24286 [==============================] - 2s 87us/step - loss: 0.2968 - acc: 0.8971 - val_loss: 0.3630 - val_acc: 0.9070\n",
      "Epoch 171/200\n",
      "24286/24286 [==============================] - 2s 79us/step - loss: 0.2924 - acc: 0.9024 - val_loss: 0.3627 - val_acc: 0.9096\n",
      "Epoch 172/200\n",
      "24286/24286 [==============================] - 2s 77us/step - loss: 0.2920 - acc: 0.9010 - val_loss: 0.3657 - val_acc: 0.9018\n",
      "Epoch 173/200\n",
      "24286/24286 [==============================] - 2s 78us/step - loss: 0.2809 - acc: 0.9023 - val_loss: 0.3774 - val_acc: 0.9066\n",
      "Epoch 174/200\n",
      " 8320/24286 [=========>....................] - ETA: 1s - loss: 0.2829 - acc: 0.9037"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(num_hidden_neurons,input_dim = X_train[0].shape[0],activation = 'relu'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(dropout))\n",
    "\n",
    "model.add(Dense(14,activation = \"softmax\"))\n",
    "\n",
    "model.compile(loss = mywloss,optimizer = \"adam\",metrics = ['acc'])\n",
    "\n",
    "model.fit(X_train, y_categorical, batch_size = 128,validation_data=(X_test,y_categorical_test), epochs=epochs, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
